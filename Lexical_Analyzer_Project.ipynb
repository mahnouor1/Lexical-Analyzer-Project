{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dyiKfR47u-A",
        "outputId": "ef181eaa-0e94-444c-afe6-174b7cff5d92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source Code:\n",
            "int x = 10; if (x > 5) x = x + 1;\n",
            "\n",
            "Tokens:\n",
            "('KEYWORD', 'int')\n",
            "('IDENTIFIER', 'x')\n",
            "('OPERATOR', '=')\n",
            "('NUMBER', '10')\n",
            "('SEPARATOR', ';')\n",
            "('KEYWORD', 'if')\n",
            "('SEPARATOR', '(')\n",
            "('IDENTIFIER', 'x')\n",
            "('OPERATOR', '>')\n",
            "('NUMBER', '5')\n",
            "('SEPARATOR', ')')\n",
            "('IDENTIFIER', 'x')\n",
            "('OPERATOR', '=')\n",
            "('IDENTIFIER', 'x')\n",
            "('OPERATOR', '+')\n",
            "('NUMBER', '1')\n",
            "('SEPARATOR', ';')\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# Define token patterns\n",
        "TOKEN_PATTERNS = [\n",
        "    (\"KEYWORD\", r\"\\b(if|else|while|for|return|int|float|char|void)\\b\"),\n",
        "    (\"IDENTIFIER\", r\"\\b[a-zA-Z_][a-zA-Z0-9_]*\\b\"),\n",
        "    (\"NUMBER\", r\"\\b\\d+(\\.\\d+)?\\b\"),\n",
        "    (\"OPERATOR\", r\"[+\\-*/=<>!]+\"),\n",
        "    (\"SEPARATOR\", r\"[(),;{}\\[\\]]\"),\n",
        "    (\"WHITESPACE\", r\"\\s+\"),\n",
        "    (\"UNKNOWN\", r\".\")\n",
        "]\n",
        "\n",
        "# Compile patterns\n",
        "TOKEN_REGEX = [(name, re.compile(pattern)) for name, pattern in TOKEN_PATTERNS]\n",
        "\n",
        "def lexical_analyzer(code):\n",
        "    tokens = []\n",
        "    position = 0\n",
        "\n",
        "    while position < len(code):\n",
        "        match_found = False\n",
        "\n",
        "        for token_name, token_regex in TOKEN_REGEX:\n",
        "            match = token_regex.match(code, position)\n",
        "            if match:\n",
        "                match_text = match.group(0)\n",
        "                if token_name != \"WHITESPACE\":  # Ignore whitespace tokens\n",
        "                    tokens.append((token_name, match_text))\n",
        "                position = match.end()\n",
        "                match_found = True\n",
        "                break\n",
        "\n",
        "        if not match_found:\n",
        "            raise ValueError(f\"Unexpected token at position {position}: {code[position]}\")\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Test the lexical analyzer\n",
        "if __name__ == \"__main__\":\n",
        "    sample_code = \"int x = 10; if (x > 5) x = x + 1;\"\n",
        "    print(\"Source Code:\")\n",
        "    print(sample_code)\n",
        "    print(\"\\nTokens:\")\n",
        "    for token in lexical_analyzer(sample_code):\n",
        "        print(token)\n"
      ]
    }
  ]
}